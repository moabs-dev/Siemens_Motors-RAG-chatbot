# Siemens Motors RAG Chatbot ğŸš€

[![Python](https://img.shields.io/badge/python-3.11-blue.svg)](https://www.python.org/)  
[![FastAPI](https://img.shields.io/badge/FastAPI-0.115+-teal.svg)](https://fastapi.tiangolo.com/)  
[![Streamlit](https://img.shields.io/badge/Streamlit-1.50+-red.svg)](https://streamlit.io/)  
[![LangChain](https://img.shields.io/badge/LangChain-0.2+-yellow.svg)](https://www.langchain.com/)  
[![Docker](https://img.shields.io/badge/Docker-Compose-blue.svg)](https://www.docker.com/)  

---

## ğŸ“Œ Overview

**Siemens Motors RAG Chatbot** is a **context-aware AI assistant** built for Siemens Motors employees.  
Instead of searching through long catalogs, employees can directly ask questions about the catalog, and the chatbot answers contextually.

This project leverages:

- **LangChain** as the backbone for orchestration  
- **FAISS** as the vector database  
- **HuggingFace embeddings** for semantic search  
- **Groq LLMs** (but also supports OpenAI and Ollama)  
- **FastAPI** for the backend  
- **Streamlit** for the frontend  
- **Docker** for containerization  

---

## ğŸ“‚ Folder Structure

```
project-root/
â”‚â”€â”€ .env                  # API keys (OpenAI, Groq, HuggingFace, etc.)
â”‚â”€â”€ docker-compose.yml    # Defines backend + frontend services
â”‚â”€â”€ LICENSE               # MIT License
â”‚â”€â”€ motors-catalog.pdf    # Catalog data used for embeddings
â”‚â”€â”€ errorFaced.txt        # Notes on encountered errors during dev
â”‚â”€â”€ outputs/              # Sample outputs & screenshots
â”‚
â”œâ”€â”€ Backend/
â”‚   â”œâ”€â”€ backend.py        # Core LLM, retriever, memory setup
â”‚   â”œâ”€â”€ backend2.py       # API endpoints (uses functions from backend.py)
â”‚   â”œâ”€â”€ back.py           # LLM chain logic (ConversationalRetrievalChain)
â”‚   â”œâ”€â”€ requirements.txt  # Backend dependencies
â”‚   â”œâ”€â”€ Dockerfile        # Backend Dockerfile
â”‚
â”œâ”€â”€ Frontend/
â”‚   â”œâ”€â”€ frontend.py       # Streamlit frontend
â”‚   â”œâ”€â”€ frontend2.py      # Modified Streamlit frontend
â”‚   â”œâ”€â”€ requirements.txt  # Frontend dependencies
â”‚   â”œâ”€â”€ Dockerfile        # Frontend Dockerfile
â”‚
â”œâ”€â”€ database_creation/
â”‚   â”œâ”€â”€ text_extract.py   # Extract text & chunk from PDF
â”‚   â”œâ”€â”€ test1.py          # Experimental database creation
â”‚
â”œâ”€â”€ faiss_index/          # Vector DB built from catalog
â”‚
â””â”€â”€ outputs/              # Validation screenshots & test runs
```

---

## âš™ï¸ Workflow

1. **Database Creation**  
   - PDF catalog is processed â†’ chunked â†’ embeddings generated using HuggingFace.  
   - FAISS index is saved inside `faiss_index/`.

2. **Backend (FastAPI)**  
   - Exposes endpoints:  
     - `/set_key` â†’ set API key (overrides `.env`)  
     - `/chat` â†’ handle user queries, return answer + sources  

3. **Frontend (Streamlit)**  
   - UI where employees set API key & chat with the bot.  
   - For each query â†’ frontend sends it to backend â†’ backend retrieves relevant chunks â†’ passes to LLM â†’ returns contextual answer.

4. **Models Supported**  
   - Groq (`ChatGroq`)  
   - OpenAI (`ChatOpenAI`)  
   - Ollama (`ChatOllama`)  

---

## ğŸ³ Running with Docker

### 1. Build & Run with Compose
```bash
docker-compose up --build
```

This will start:
- **Backend** â†’ http://localhost:8000  
- **Frontend** â†’ http://localhost:8501  

### 2. Using Pre-built Images
Pull directly from Docker Hub:

```bash
docker pull moeenabbas/rag_on_seimens_catalog-backend:latest
docker pull moeenabbas/rag_on_seimens_catalog-frontend:latest
```

Then run:
```bash
docker-compose up -d
```

---

## ğŸ› ï¸ Setup without Docker (Development Mode)

1. Clone the repository:
```bash
git clone https://github.com/moabs-dev/siemens-motors-rag-chatbot.git
cd siemens-motors-rag-chatbot
```

2. Setup Backend:
```bash
cd Backend
pip install -r requirements.txt
uvicorn backend2:app --reload
```

3. Setup Frontend:
```bash
cd ../Frontend
pip install -r requirements.txt
streamlit run frontend2.py
```

---

## ğŸ”‘ Environment Variables

Add a `.env` file in the root with your keys:
```env
OPENAI_API_KEY=your_openai_key_here
GROQ_API_KEY=your_groq_key_here
HUGGINGFACEHUB_API_TOKEN=your_hf_key_here
```

---

## ğŸ“¸ Outputs

Screenshots & validation examples are inside the `outputs/` folder:
- Valid API key test  
- Example Q&A workflow  
- Error handling for invalid key  

---

## ğŸ§© Customization

- Replace `motors-catalog.pdf` with your own documents.  
- Re-run `database_creation/text_extract.py` to regenerate FAISS index.  
- Swap models in `backend.py` (`ChatGroq`, `ChatOpenAI`, or `ChatOllama`).  
- Extend memory type, chain type (`stuff`, `map_reduce`, etc.) depending on complexity.  

---

## ğŸ“„ License

This project is licensed under the **MIT License** â€“ free to use, modify, and distribute.

---

## ğŸ™Œ Credits

Developed by **Moeen Abbas**  
Built with **LangChain, HuggingFace, Groq, OpenAI, FastAPI, Streamlit, and Docker**.
